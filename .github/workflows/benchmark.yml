name: Process Benchmark

on:
  workflow_dispatch:
    inputs:
      benchmark_data:
        description: 'Base64 encoded benchmark data'
        required: true

jobs:
  store-and-visualize:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Decode benchmark data
        run: echo "${{ github.event.inputs.benchmark_data }}" | base64 -d > pytest_benchmark.json

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Convert benchmark format
        run: |
          cat > convert_benchmark.py << 'EOF'
          #!/usr/bin/env python3
          import json
          import sys

          # Load pytest-benchmark JSON
          with open('pytest_benchmark.json', 'r') as f:
              pytest_data = json.load(f)

          # Create custom format with time and memory benchmarks
          custom_benchmarks = []
          for benchmark in pytest_data.get('benchmarks', []):
              # Extract data
              raw_name = benchmark.get('name', '')
              is_memory = '_memory' in raw_name
              # Clean up the name by removing suffixes
              name = raw_name.replace('_time', '').replace('_memory', '')

              if is_memory:
                  # Memory benchmark
                  memory_mb = benchmark.get('stats', {}).get('mean', 0)
                  custom_benchmarks.append({
                      'name': name + ' - Memory',
                      'value': memory_mb,
                      'unit': 'MB',
                      'range': '0',
                      'extra': benchmark.get('extra_info', {})
                  })
              else:
                  # Time benchmark
                  time_value = benchmark.get('stats', {}).get('mean', 0)
                  time_stddev = benchmark.get('stats', {}).get('stddev', 0)
                  custom_benchmarks.append({
                      'name': name + ' - Time',
                      'value': time_value,
                      'unit': 'seconds',
                      'range': str(time_stddev),
                      'extra': benchmark.get('extra_info', {})
                  })

          # Write custom JSON
          with open('benchmark.json', 'w') as f:
              json.dump(custom_benchmarks, f, indent=2)
          EOF

          python convert_benchmark.py

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customSmallerIsBetter'
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          comment-on-alert: true
          fail-on-alert: true
          alert-threshold: '120%'
          max-items-in-chart: 100
