\documentclass[letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{soul}
% \usepackage[legalpaper, margin=2in]{geometry}
\parindent0pt
\parskip 1.5ex plus 1ex minus .5ex

\definecolor{Blue}{rgb}{0,0,1}                                                     
\definecolor{Red}{rgb}{1,0,0}                                                      
\definecolor{Green}{rgb}{0,1,0}                                                    
\definecolor{Bronze}{rgb}{0.8,0.5,0.2}                                             
\definecolor{Violet}{rgb}{0.54,0.17,0.89}                                          
                                                                                   
\newcommand{\TODO}[1]{{\textcolor{Violet}{TODO: #1}}}                              
\newcommand{\YC}[1]{{\textcolor{Bronze}{#1}}}                                     
\newcommand{\SG}[1]{{\textcolor{Blue}{#1}}}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\Ell}{\mathcal{L}}
\newcommand{\R}{\mathds{R}}
\newcommand{\C}{\mathds{C}}


\title{Quandary: Optimal Control for Quantum Systems}
% \author{Stefanie G{\"u}nther}
% \affil{Lawrence Livermore National Laboratory, CA, USA}
% \date{}

\begin{document}
\maketitle


Quandary numerically simulates and optimizes the time-evolution of open quantum systems. The underlying dynamics are modelled by Lindblad's master equation, a linear ordinary differential equation (ODE) describing quantum systems interacting with the environment. Quandary solves this ODE numerically by applying a time-stepping integration scheme, and utilizes gradient-based numerical optimization approaches to determine optimal control pulses that drive the quantum system to a desired target state. 
Two optimization objectives are considered: (a) Unitary gate optimization that finds controls to realize a unitary gate transformation, and (b) optimal reset that aims to drive the quantum system to the ground states. 
Gradient-based optimization schemes utilizing Petsc's Tao optimization package are applied to generate control pulses that minimize the respective measure. 
To mitigate excessive execution run times, Quandary can be build to link with the XBraid software library which provides a parallelization strategy to distribute the time-evolution of the underlying dynamics onto multiple processor applying a parallel-in-time multigrid reduction scheme. 


\section{Model equation}
Quandary models open quantum systems with $Q$ subsystems with $n_k$ levels for the
$k$-th subsystem, $k=0,\dots,Q-1$. The time-evolution of the open quantum system is modeled by Lindblad's master equation:
\begin{align}\label{mastereq}
  \dot \rho(t) = &-i(H(t)\rho(t) - \rho(t)H(t)) \notag \\
  &+ \sum_{l=1}^2 \sum_{k=0}^{Q-1} \gamma_{lk} \left( \Ell_{lk} \rho(t)
  \Ell_{lk}^{\dagger} - \frac 1 2 \left( \Ell_{lk}^{\dagger}\Ell_{lk}
  \rho(t) + \rho(t)\Ell_{lk}^{\dagger} \Ell_{lk}\right) \right)
\end{align}
for the density matrix $\rho(t)\in \C^{N\times N}$ with dimension $N :=
\prod_{k=0}^{Q-1} n_k$. The Hamiltonian $H(t)$ consists of a constant \textit{drift}
part, and a time-varying \textit{control} part. For the computational
\textit{rotating frame} with fundamental frequencies $\omega_k$ and rotation
frequency $\omega_k^{\text{rot}}$, these are computed from
\begin{align}
  H(t) &= H_d + H_c(t) \\
  \text{with} \quad H_d &:= \sum_{k=0}^{Q-1} \left(\left(\omega_k -                                 
  \omega_k^{\text{rot}}\right)a_k^{\dagger}a_k- \frac{\xi_k}{2}
  a_k^{\dagger}a_k^{\dagger}a_k a_k - \sum_{l> k} \xi_{lk}
  a_{k}^{\dagger}a_{k}
  a_{l}^{\dagger} a_{l} \right) \\
   H_c(t) &:= \sum_{k=0}^{Q-1} \left( p^k(\vec{\alpha}^k,t) (a_k +
   a_k^{\dagger}) + i q^k(\vec{\alpha}^k,t)(a_k - a_k^{\dagger})
   \right)
\end{align}
Here, $a_k$ denotes the lowering operator:
\begin{align}
  \begin{array}{rl}
  a_0 &:= a^{(n_0)} \otimes I_{n_1} \otimes \dots \otimes
  I_{n_{Q-1}}\\
  a_1 &:= I_{n_0} \otimes a^{(n_1)} \otimes \dots \otimes
  I_{n_{Q-1}}\\
  \vdots \, & \\
  a_{Q-1} &:= I_{n_0} \otimes I_{n_1} \otimes \dots \otimes
  a^{(n_{Q-1})}\\
  \end{array}
  \quad \text{with}\quad
 a^{(n_k)} = \begin{pmatrix}
   0 & 1 &          &         &    \\
     & 0 & \sqrt{2} &         &     \\
     &   & \ddots   & \ddots  &    \\
     &   &          &         & \sqrt{n_k-1}  \\
     &   &          &         & 0   
 \end{pmatrix} \in \R^{n_k \times n_k}
\end{align}
and $I_{n_k}$ denotes the identity matrix in $\R^{n_k \times n_k}$.

\subsection{Collapse operators}
The collapse operators $\Ell_{lk}$ in the Lindblad terms of the master equation
\eqref{mastereq} can be of the following type:
\begin{itemize}
  \item ``T1'' -- Decay: $\Ell_{1k} = a_k$
  \item ``T2'' -- Dephasing: $\Ell_{2k} = a_k^{\dagger}a_k$
\end{itemize}
for each subsystem $k$. The constants $\gamma_{lk}$ are the inverse half-life
for the corresponding collapse process $l$: $\gamma_{lk} = {\frac{1}{T_l}}$.
Typical T1 decay time is between $10-100$ microseconds (us). T2
dephasing time is typically about half of T1 decay time. Decay then behaves
like $\exp(-t/{T_1})$. 

\subsection{Control pulses} \label{subsec:controlpulses}
The time-dependent control functions $p^k(\vec{\alpha}^k,t),
q^k(\vec{\alpha}^k,t)$ are real-valued. They are discretized using piecewise quadratic B-spline basis functions with
carrier waves: 
\begin{align}
  p^k(\vec{\alpha}^k,t) &= \sum_{l=0}^{L-1} B_l(t)
  \sum_{f=0}^{N_f^k-1} \left(\alpha^{k
  (0)}_{l,f} \cos(\Omega_f^k t) - \alpha^{k (1)}_{l,f} \sin(\Omega_f^k t)
  \right) \\
  q^k(\vec{\alpha}^k,t) &= \sum_{l=0}^{L-1} B_l(t)
  \sum_{f=0}^{N_f^k-1} \left( \alpha^{k
  (0)}_{l,f} \sin(\Omega_f^k t) + \alpha^{k (1)}_{l,f} \cos(\Omega_f^k t)
  \right)
\end{align}
for $L$ B-Spline functions $B_l(t)$, and $N_f^k$ carrier wave frequencies
$\Omega_f^k$ per oscillator. The amplitudes $\vec{\alpha}^k \in \R^{2LN_f^k}$ are the control
parameters (\textit{design} variables) that Quandary can optimize in order to realize a
desired system behavior. 

These control functions are in the \textit{rotating frame}. To convert them back
to the \textit{Lab frame}, Quandary uses
\begin{align}
  f^k(t) = 2 \sum_{l=0}^{L-1} B_l(t) \sum_{f=0}^{N_f^k-1} \beta_{l,f}^k \cos(w_k t +
  \Omega_f^k t + \theta_{l,f}) \quad \forall k=0,\dots Q-1
\end{align}
where $\beta_{l,f}^k$ and $\theta_{l,f}$ are computed from
\begin{align}
  \alpha_{l,f}^{k(0)} = \beta_{l,f}^k \cos(\theta_{l,f}) \quad \text{and} \quad
  \alpha_{l,f}^{k(1)} = \beta_{l,f}^k \sin(\theta_{l,f})
\end{align}
and $w_k$ are the fundamental resonance frequencies of subsystems $k$. Using
trigonometric equalities, the Lab-frame controls can be expressed as
\begin{align}
  f^k(t) &= 2 \sum_{l=1}^L B_l(t) \sum_{f=1}^{N_f^k} \alpha_{l,f}^{k(0)} \cos((w_k
  + \Omega_f^k) t) - \alpha_{l,f}^{k(1)}\sin((w_k + \Omega_f^k) t) \quad \forall
  k=0,\dots Q-1 \\
         &= 2 p^k(\vec{\alpha}^k, t) \cos(w_k t) - 2 q^k(\vec{\alpha}^k,
         t)\sin(w_k t) \\
\end{align}


\section{Optimization problem}
In the most general form, Quandary can solve the following optimization problem
\begin{align}
  \min_{\boldsymbol{\alpha}} \frac{1}{ninit} \sum_{i=1}^{ninit} \beta_i J(\rho_i(T))  + R(\boldsymbol{\alpha})
\end{align}
where $\rho_i(T)$ solves the master equation \eqref{mastereq} for initial conditions $\rho_i(0)$, as explained below. Specific coices of $J$ and initial conditions $\rho_i(0)$ are given below, as well as regularization and penalty terms denoted by $R$ above. 

\subsection{Groundstate optimization aka optimal
reset}\label{sec:groundstate-obj}
Optimal reset aims to find control pulses $p^k(\vec{\alpha}^k, t), q^k(\vec{\alpha}^k, t)$,
that drive any initial state towards the ground state $|0\dots 0\rangle = (1, 0,
0, \dots )^T$. The corresponding ground state density matrix is given by
\begin{align}
  \rho_{G} := |0\dots 0\rangle \langle 0 \dots 0 | = 
  \begin{bmatrix} 1      & 0      &  \dots   \\ 
                  0      & 0      &  \dots  \\ 
                  \vdots & \vdots &  \dots 
  \end{bmatrix}
\end{align}

To reach this goal, Quandary either minimizes the distance between the final state
density matrix and the ground-state density matrix, measured in the frobenius
norm, or minimizes a weighted average of the expected energy level over each subsystem:
\begin{enumerate}
  \item \textit{Expected energy minimization}: Minimize a weighted sum over
    expected energy levels of the (full or partial) system
    \begin{align}
       \quad J(\rho(T)) = \sum_{k\in \mathcal{K}} \langle O^k \rangle 
    \end{align}
    using the observable $O^k = a_k^\dag a_k$ for the subsystem $k$, and defining a partial system
    $H_{k_0}\otimes \dots \otimes H_{k_s}$ from an index set of
    subsystem IDs $\mathcal{K}=\{k_0,\dots,k_s\}\subset
    \{0,\dots,Q-1\}$. 
    $O^k$ measures the probability of the energy levels of subsystem $k$. The
   expected energy level of subsystem $k$ is hence given by 
  \begin{align}
    \langle O^k \rangle &= \mbox{Tr}(O^k\rho).
    \label{eq:expected_energy1}
  \end{align}
    
    Quandary can use the following variations of the
    above measure
    \begin{enumerate}
      \item[(a)] $J(\rho(T)) =
        \left(\frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\langle O^k \rangle
        \right)^2$
      \item[(b)] $J(\rho(T)) =
        \frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\left(\langle O^k \rangle
        \right)^2 $
      \item[(c)] $J(\rho(T)) =
        \frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\langle O^k \rangle$
    \end{enumerate}
  \item \textit{Direct ground-state optimization}: Minimize the frobenius norm
    between the final density matrix and the groundstate density matrix:
    \begin{align}\label{eq:ground-state-obj}
      J(\rho(T)) = \frac 12 \| \rho(T) - \rho_G \|^2_F 
    \end{align}
    where the Frobenius norm is defined as $\|A\|^2_F = \Tr(A^{\dagger}A)$.
\end{enumerate}

\subsection{Target gate optimization}

Here, the goal is to find control pulses that realize a certain logical gate
operation at final time $T$. Representing the gate by a unitary matrix $V\in
\C^{N\times N}$, the goal is to match the final state $\rho(T)$ to the unitary
transformation $V\rho(0)V^{\dagger}$ for any initial state $\rho(0)$. Two different measures are available:

\begin{align}
 \text{Frobenius norm:} \quad & J(\rho(T)) = \frac{1}{2}\| \rho(T) - V\rho(0)V^{\dagger} \|^2_F  \\
 \text{Trace overlap:} \quad  & J(\rho(T)) = 1 - \mbox{Tr}\left(V\rho(0)V^{\dagger}\rho(T) \right) 
\end{align} 


Target gates that are currently implemented are
\begin{align}
  V_{X} := \begin{bmatrix} 0 & 1 \\ 1 & 0  \end{bmatrix} \quad
  V_{Y} := \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} \quad
  V_{Z} := \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \quad 
  V_{Hadamard} := \frac{1}{\sqrt{2}} 
           \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \\
  V_{CNOT} := \begin{bmatrix} 1  & 0 & 0 & 0 \\ 
                               0  & 1 & 0 & 0 \\ 
                               0  & 0 & 0 & 1 \\ 
                               0  & 0 & 1 & 0 \\ 
                \end{bmatrix}
\end{align}

An average gate fidelity is computed from 
\begin{align}
  F_{avg} = \frac{1}{N^2} \sum_{i=1}^{N^2} \mbox{Tr}\left(V\rho_i(0)V^{\dagger}\rho_i(T) \right) 
\end{align}
where $\rho_i(0) = B^{k(i), j(i)}$ are the basis initial states and $\rho_i(T)$ their corresponding propagated final time states.

\subsection{Penalty and regularization term}
In order to stabilize the optimization problem, a Tikhonov regularization term can be added to the objective function:
\begin{align}
 \frac{\gamma_1}{2} \| \boldsymbol{\alpha} \|^2_2
\end{align}
with Tikhonov parameter $\gamma_1 > 0$, where $\boldsymbol{\alpha}$ denotes the vector of design parameters as in section \ref{subsec:controlpulses}.


In addition, when considering ground-state optimization in
Section~\ref{sec:groundstate-obj}, an integral penalty term can be added to the objective function that discourages non-zero energy states over time:
\begin{align}
  % \int_0^T w(t) J(\rho(t)) \, \mathrm{d}t, \quad \text{where} \quad w(t) =
  % \left(\frac{t}{T}\right)^p, p \geq 1
  \gamma_2 \int_0^T w(t) J\left(\rho(t)\right) \, \mathrm{d}t, \quad \text{where} \quad w(t) =
  \frac{1}{a} e^{ -\left(\frac{t-T}{a} \right)^2},
\end{align}
for a penalty parameter $\gamma_2 > 0$, and parameter $0 < a \leq 1$. 
Note, that as $a\to 0$, the weight $w(t)$ converges to the dirac delta
distribution with peak at final time $T$. 



\subsection{Initial conditions}\label{subsec:initcond}

\subsubsection{Pure state initialization}
One can choose to simulate and optimize for only one specific initial condition $\rho(0)$. In that case, each evaluation of the objective function solves the master equation once for this specific initial condition ($\Rightarrow ninit = 1$). 

In the case of pure-state initialization, the initial density matrix is composed
of kronecker products of pure states for each of the subsystems. E.g. for a bipartite system with $n_1
\otimes n_2$ levels, one can propagate any initial pure state 
\begin{align}
  \rho(0)  = |a\rangle \langle a| \otimes |b\rangle \langle b| \quad \text{for} \quad a \in \{0,\dots, n_1-1\}, b\in \{0,\dots, n_2-1\}
\end{align}
Alternatively, a specific initial condition can also be read from a file. 

The unique integer identifier for
this initial condition will be $i=-1$ (in order to distinguish from the below
basis elements or diagonals as below). 

\subsubsection{Basis for initial condition density matrices}
Typically, the optimization goal is to find control parameters that minimize the
respective measure for \textit{any} possible initial condition $\rho(0)$. We
therefore define a basis for all initial density matrices and aim to minimize
the average of the above measure over those matrix basis elements. The basis
matrices are defined as follows:

\begin{align}
B^{kj} := \frac 12 \left( E_{kk} + E_{jj}\right) +  \begin{cases} 
          0 & \text{if} \, k=j \\ 
        \frac 12 \left( E_{kj} + E_{jk}\right) & \text{if} \, k<j \\
        \frac i2 \left( E_{jk} - E_{kj}\right) & \text{if} \, k>j
      \end{cases} 
\end{align}
for all $k,j\in\{0,\dots, N-1\}$, where $E_{kj}$ denotes the matrix of all zeros
except for the $(k,j)$-th element which is one (hence $E_{kj} = e_ke_j^T$).
These $N^2$ matrices are Hermitian, and they are linearly independent over the
real vector space of Hermitian matrices. They further satisfy $\Tr(B^{kj}) = 1$
and they are positive semi-definite, hence each basis matrix is a density
matrix. 

In Quandary, these $N^2$ basis elements can be used to perform the optimization ($\Rightarrow ninit=N^2$). To uniquely identify the different initial conditions in the code, we assign a
unique index $i \in \{0,\dots, N^2-1\}$ to each basis elements with 
\begin{align*}
  B^i := B^{k(i), j(i)}, \quad \text{with} \quad k(i) := i \,\mbox{mod}\, N,
  \quad \text{and} \quad j(i) := \left\lfloor \frac{i}{N} \right\rfloor
\end{align*}

% We can rewrite any initial condition $\rho(0)$ as a linear combination 
% \begin{align}
%   \rho(0) = \sum_{k=0}^{N-1} \sum_{j=0}^{N-1} a_{kj} B^{k, j}.
% \end{align}
% for coefficients $a_{kj} \in \R$.
% Since the master equation \eqref{mastereq} is linear, we can formally define a
% linear solution operator $S^{\vec{\alpha}}(t,\rho(0))$, mapping an initial state
% to the state at time $t$, and we can rewrite the final density matrix at time
% $T$ in terms of the basis elements:
% \begin{align}
%   \rho(T) = S^{\vec{\alpha}}(T,\rho(0)) = \sum_{k,j=0}^{N-1} a_{kj}
%   S^{\vec{\alpha}}(T,B^{k, j}).
% \end{align}
% % AP: According to D.~Lidar's lectue notes, eqn. (112), the most general
% expression for the evolution of the density matrix follows from the Kraus
% operator sum representation,
% \[
%   \rho(T) = \sum_{\mu,\nu} K_{\mu\nu}(t) \rho(0) K_{\mu\nu}^\dagger(t),\quad
%   \sum_{\mu,\nu}
%   K_{\mu\nu}^\dagger(t)K_{\mu\nu}(t)=I.
% \]

% We therefore define the optimization problem that minimizes the respective
% measure averaged over all the basis elements:
% \begin{align}\label{optimproblem_basis}
%   \min \frac{1}{N^2} & \sum_{k,j=0}^{N-1} \, J(\rho^{k,j}(T))  \\
%   \text{s.t.} \quad  \text{each} \quad \rho^{k,j}(t) \quad & \text{solves
%   \eqref{mastereq} with initial condition} \quad \rho^{k,j}(0) = B^{k,j}
% \end{align}

% If the objective measure $J(\rho)$ is linear in $\rho$ (as for example in the
% groundstate optimization minimizing the expected energy levels), we can make use
% of the linearity of $S^{\vec{\alpha}}$:
% \begin{align}
%    \frac{1}{N^2}  \sum_{k,j=0}^{N-1} \, J\left(\rho^{k,j}(T)\right) =
%    J\left(S^{\vec{\alpha}}\left(T,\frac{1}{N^2}  \sum_{k,j=0}^{N-1}
%    B^{kj}\right)\right)
% \end{align}
% Hence, we define a new optimization problem using the specific initial condition
% that averages over the basis matrices:
% \begin{align}
%   \min \, J(S^{\vec{\alpha}}(T,\rho(0))) \quad \text{s.t.} \quad \rho(0) =
%   \frac{1}{N^2}  \sum_{k,j=0}^{N-1} B^{kj}
% \end{align}
% In this case, each objective function evaluation requires to propagate only one
% initial condition $\rho(0)$. 

\subsubsection{Diagonals as initial condition}
One can choose to propagate only those basis elements that correspond to pure states. In that case, only the basis matrices that correspond
to diagonal elements ($B^{mm}, m=1,\dots,N)$ are propagated through the time
domain, and the objective function evaluates the average measure over those $N$
initial conditions at the final time $T$ ($\Rightarrow ninit = N$).

% \subsection{Fidelity} 
% For closed systems, the $\rho(t)$ and the solution operator $U(t)$ are unitary. In that case, one can show that 
% \begin{align}
%   \frac{1}{2N^2}\|\rho(T) - V\rho(0)V\|_F^2 = 1 - \frac{1}{N^2}|\Tr(U(T)^{\dagger}V)|^2,
% \end{align}
% which is called the \textit{infidelity}, and the trace-term is the \textit{fidelity}. However, since we include the Lindblad terms, $\rho(T)$ is not unitary, and the above equation is not valid anymore. For defining a fidelity we consider $F(z) := 1 - J(z)$. Note, however, that if the optimized objective is in the order $O(\delta)$, then the error $\epsilon_i$ of a final state component $i$ from the gate is rather of the order $O(\sqrt{\delta})$, since $J$ represents the average squared error $J = \frac{1}{N^2}\sum_i \epsilon_i^2$ and so $\epsilon_i^2 \sim O(\delta)$ on average.

\subsection{Three-state initialization}
According to Koch et al. it is sufficient to utilize only three initial condition to describe a unitary gate, independent of the dimension $N$. Therefore, if one optimizes for realizing a unitary gate, the following three initial states can be chosen to be considered in Quandary:
\begin{align}
    \rho(0)_0 &= \sum_{i=0}^{N-1} \frac{2(N-i+1)}{N(N+1)} |i\rangle\langle i|\\
    \rho(0)_1 &= \sum_{ij=0}^{N-1} \frac{1}{N} |i\rangle\langle j|\\
    \rho(0)_2 &= \frac{1}{N} I_N
\end{align}


\subsection{Reading an initial condition from file}
A specific initial condition can be read from file ($\Rightarrow ninit=1$ with unique identifier $-1$). Format: one column being the vectorized density matrix, first all real parts, then all imaginary parts. 

\section{Implementation}

  \subsection{Vectorization}
  The Lindblad master equation \eqref{mastereq} is in matrix form, describing
  the evolution of the densisy matrix $\rho = (\rho_1, \dots, \rho_N) \in
  \C^{N\times N}$. In order to solve this numerically, we vectorize the equation
  to receive an ODE for $q(t) := \text{vec}(\rho(t)) \in \C^{N^2}$. Using the
  relations
  \begin{align}
   \text{vec}(AB) &= (I_N\otimes A)\text{vec}(B) = (B^T\otimes I_N)\text{vec}(A)
    \\
   \text{vec}(ABC) &= (C^T\otimes A)\text{vec}(B)
  \end{align}
  for square matrices $A,B,C\in\C^{N\times N}$, we can derive the vectorized
  form of the Lindblad master equation:
  \begin{align}\label{mastereq_vectorized}
    &\dot q(t) = M(t) q(t) \quad  \text{where} \\
    &M(t) := -i(I_N\otimes H - H^T \otimes I_N) + \sum_{k=0}^{Q-1}\sum_{l=1}^2 \gamma_{lk}
    \left( \Ell_{lk}\otimes \Ell_{lk} - \frac 1 2 \left( I_N\otimes
    \Ell^T_{lk}\Ell_{lk} + \Ell^T_{lk}\Ell_{lk} \otimes I_N \right) \right)
  \end{align}

  \subsection{Real-valued system}
   Quandary solves the vectorized master equation \eqref{mastereq_vectorized} in
   real-valued variables with $q(t) = u(t) + iv(t)$, evolving the real-valued
   states $u(t), v(t)\in \R^{N^2}$ with
   \begin{align}
     \dot q(t) = M(t) q(t) \quad \Leftrightarrow \quad \begin{bmatrix} \dot u(t) \\ \dot v(t) \end{bmatrix} = 
   \begin{bmatrix} A(t) & -B(t) \\ B(t) & A(t) \end{bmatrix} 
   \begin{pmatrix} u(t) \\ v(t) \end{pmatrix} 
   \label{realvaluedODE}
   \end{align}
   where $M(t) = A(t) + i B(t)$, $A(t), B(t)\in \R^{N^2\times N^2}$. To assemble
   $A = Re(M)$ and $B = Im(M)$ consider
   \begin{align}
     -i(I_N \otimes H - H^T \otimes I_N) &= -I_N \otimes \left(iH_d +
     iH_c(t)\right) + \left(iH_d + iH_c(t)\right)^T \otimes I_N \\
     \text{and} \quad iH_d + iH_c(t) &= i H_d + i\left( \sum_k
     p^k(\vec{\alpha}^k,t)(a_k + a_k^{\dagger}) + iq^k(\vec{\alpha}^k,t)(a_k -
     a_k^{\dagger})\right) \\
                    &= - \sum_k q^k(\alpha^k,t)(a_k - a_k^{\dagger}) + i\left(
                    H_d + \sum_k p^k(\alpha^k,t)(a_k+a_k^{\dagger}) \right) 
   \end{align}
   Hence $A$ and $B$ consist of a constant part $A_d, B_d$ and a time-varying
   part $A_c(t), B_c(t)$ with the following:
   \begin{align}
     A_d &=  \sum_{l,k}\gamma_{lk} \left( \Ell_{lk}\otimes\Ell_{lk} -
     \frac 1 2 \left(I_N \otimes \Ell_{lk}^T\Ell_{lk} +
     \Ell_{lk}^T\Ell_{lk}\otimes I_N\right) \right)\\
     B_d &= -I_N \otimes H_d + H_d^T \otimes I_N \\
     A_c(t) &= \sum_k q^k(\vec{\alpha}^k,t) \underbrace{\left( I_N \otimes
     \left(a_k - a_k^{\dagger}\right) - \left(a_k -
     a_k^{\dagger}\right)^T\otimes I_N \right)}_{=:A_c^k} \\
     B_c(t) &= \sum_k p^k(\vec{\alpha}^k,t) \underbrace{\left( - I_N \otimes
     \left(a_k + a_k^{\dagger}\right) + \left(a_k +
     a_k^{\dagger}\right)^T\otimes I_N \right)}_{=:B_c^k} 
   \end{align}

   \subsection{Sparse-matrix vs. matrix-free solver}

   In Quandary, two versions to evaluate the right hand side of Lindblad's
   equation, $M(t)q(t)$, of the vectorized real-valued system are available:
   \begin{itemize}
     \item \textit{Sparse-matrix solver:}
      The sparse-matrix solver initializes and stores the constant matrices
       $A_d,B_d, A_c^k, B_c^k$ using Petsc's sparse-matrix format. They are used
       as building blocks to evaluate the sparse system matrix $M(t)$ with 
     \begin{align}
       A(t) &= Re(M(t)) = A_d + \sum_kq^k(\alpha^k, t)A_c^k \\
       B(t) &= Im(M(t)) = B_d + \sum_k p^k(\alpha^k, t)B_c^k
     \end{align}
   at each time $t$. $M(t)$ is then multiplied to the vectorized density matrix
       $q(t)$ using Petsc's sparse MatVec implementation. 

   \item \textit{Matrix-free solver using tensor contractions:}
     Then tensor-contraction code is matrix-free, i.e. the matrices $A_d,B_d,
       A_c^k, B_c^k$ and hence $M(t)$ are not stored explicitly, but instead
       their action on a vector $q(t)$ is evaluated at each time step $t$. The
       action of those matrices to the system vector is implemented using
       tensor-contractions applied to the corresponding dimension of the density
       matrix. 

     \textbf{Notes}:
     \begin{itemize}
       \item For our current test cases, the matrix-free solver is much faster
         than the sparse-matrix solver (about 10x). However the matrix-free solver
         is currently only implemented for systems consisting of \textbf{two}
         subsystems with $n_1 \otimes n_2$ levels. Future development will
         extend the approach to more than two subsystem. 
       \item The matrix-free solver currently does not parallelize across the
         system dimension (i.e. no parallel Petsc!).
     \end{itemize}


   \end{itemize}

  \subsection{Co-located storage of real and imaginary part}
  The real and imaginary parts of $q(t)$ are stored in a co-located manner: For
  $q = u+iv$ with $u,v\in\R^{N^2}$, a vector of size $2N^2$ is stored that
  staggers real and imaginary parts behind each other for each component:
  \begin{align*}
    q = u+iv = \begin{bmatrix}
     u^1\\u^2\\ \vdots \\ u^{N^2-1} 
    \end{bmatrix}
    + i \begin{bmatrix}
     v^1\\v^2\\ \vdots \\ v^{N^2-1} 
    \end{bmatrix}
    \quad \Rightarrow \quad
    q_{store} = \begin{bmatrix}
      u_1 \\ v_1\\ u_2 \\ v_2 \\ \vdots \\ u_{N^2-1} \\ v_{N^2-1}
    \end{bmatrix}
  \end{align*}
  In order to access the real and imaginary parts of the $i^{th}$ component of
  $q$, one has to access the elements of $q_{store}$ at index $2i$ and $2i+1$,
  respectively. 

  \subsection{Time-stepping}
    To solve the vectorized master equation \eqref{mastereq_vectorized}, $\dot
    q(t) = M(t) q(t)$ for $t\in [0,T]$, Quandary applies a time-stepping integration
    scheme on a uniform time discretization grid $0=t_0 < \dots t_{N} = T$, with
    $t_n = n \delta t$ and $\delta t = \frac{T}{N}$, and approximates the
    solution at each discrete time step $q^{n} \approx q(t_n)$. The implicit midpoint rule is the default setting, and is the preferred time-stepping integration scheme. 
   
    \subsubsection{Implicit Midpoint Rule (IMR)} 
    The implicit mid-point rule is a simplectic, second-order integration scheme
    of Runge-Kutta type, with a Runge-Kutta tableau given by
    \begin{tabular}{ c | c }
      $1/2$ & $ 1/2$ \\
      \hline
                &  $1$
    \end{tabular}.
    Given a state $q^n$ at time $t_n$, the update formula to compute $q^{n+1}$
    is hence 
    \begin{align}
      q^{n+1} = q^n + \delta t k_1 \quad \text{where} \, k_1 \, \text{solves}
      \quad \left( I-\frac{\delta t}{2} M^{n+1/2} \right) k_1 = M^{n+1/2}  q^n
    \end{align}
    where $M^{n+1/2} := M(t_n + \frac{\delta t}{2})$. In each time-step, we
    first solve a linear equation to get the stage variable $k_1$, then use it
    to update $q^{n+1}$. 

    The consistent discrete adjoint (backwards) time-integration step for
    adjoint variables $\bar q^{n+1}, \bar q^n$, and the gradient update for this
    step are given by
    \begin{align}
      \bar q^{n} = \bar q^{n+1} + \delta t \left(M^{n+1/2}\right)^T \bar k_1
      \quad \text{where} \, \bar k_1 \, \text{solves} \quad \left(
      I-\frac{\delta t}{2} M^{n+1/2}\right)^T  \bar k_1 = \bar q^{n+1} 
    \end{align}
    The contribution to the reduced gradient $\nabla J$ for each time step is
    then given by
    \begin{align}
      \nabla J += \delta t \left( \frac{\partial M^{n+1/2}}{\partial z}
      \left(q^n + \frac{\delta t}{2} k_1\right) \right)^T\bar k_1
    \end{align}


    \subsubsection{Other integrators}
    The code offers the possibility to apply any time-stepping integrators
    provided by the Petsc software package (class TS). Implementation needs to
    be verified. Adjoint time-stepping with Petsc is currently not
    implemented. 

    \subsubsection{Choice of the time-step size}
    In order to choose a time-step size $\delta t$, an eigenvalue analysis of
    the constant drift Hamiltonian $H_d =  -2\pi \left(\sum_{k=0}^{Q-1} \frac{x_k}{2}
    a_k^{\dagger}a_k^{\dagger}a_ka_k + \sum_{l>k} x_{lk}
    a_{k}^{\dagger}a_{k}
    a_{l}^{\dagger}a_{l}\right)$ is considered:
       \begin{align*}  
         \dot u = -i H_d u \qquad \text{with} \quad H_d^{\dagger}  = H_d
       \end{align*} 
       Since $H_d$ is Hermitian, there exists a transformation $Y$ s.t. 
       \begin{align*}
         Y^{\dagger}H_d Y = \Lambda \qquad  \text{where} \quad Y^{\dagger} = Y
       \end{align*}
       where $\Lambda$ is a diagonal matrix containing the eigenvalues of $H_d$.
       Transform $\tilde u = Y^{\dagger} u$, then the ODE transforms to 
       \begin{align*}
         \dot \tilde u = -i \Lambda \tilde u \quad \Rightarrow \dot \tilde u_i =
         -i\lambda_i \tilde u_i \quad \Rightarrow \tilde u_i = a
         \exp(-i\lambda_i t)
       \end{align*}
       Therefore, the period for each mode is $\tau_i =
       \frac{2\pi}{|\lambda_i|}$, hence the shortest period is $\tau_{min} =
       \frac{2\pi}{\max_i\{|\lambda_i|\}}$. If we want $p$ discrete time points
       per period, then $p\delta t = \tau_{min}$, hence 
       \begin{align*}
         \delta t = \frac{\tau_{min}}{p} = \frac{2\pi}{p\max_i\{|\lambda_i|\}}
       \end{align*}
       Usually, for a first order scheme we would use something like $p=20$,
       second order may use $p=10$. 

       If we want to include the time-varying Hamiltonian part $H = H_d +
       H_c(t)$ in the analysis, then we could use the constraints on the control
       parameter amplitudes to remove the time-dependency using their large
       value instead, and then do the same analysis as above. However this
       doesn't ensure that we resolve the time-scale of the fastest control
       function. 
      
\subsection{Optimization}

\subsubsection{Optimization variables and bounds}

Design parameters (optimization variables) are storaged in the Quandary code in the following order: List oscillators first $(\vec{\alpha}^1, \dots, \vec{\alpha}^Q)$, then for each $\vec{\alpha}^k \in
\R^{2LN_f^k}$ iterate over $L$ splines: $\vec{\alpha}^k =
(\alpha^k_1,\dots, \alpha^k_{L})$ with $\alpha^k_l \in \R^{2N_f^k}$, then each
$\alpha^k_l$ iterates over carrierwaves and for each carrierwaves lists
the two components: $\alpha^k_l = \alpha^{k(1)}_{l,1}, \alpha^{k(2)}_{l,1},
\dots \alpha^{k(1)}_{l,N_f^k}, \alpha^{k(2)}_{l,N_f^k}$. Hence there are a total of $2QL\sum_k N_f^k$ optimization parameters, which are stored in the following order:
  \begin{align}
    \boldsymbol{\alpha} &:= \left( \vec{\alpha}^1, \dots, \vec{\alpha}^Q \right), \in
    \mathds{R}^{2QL\sum_k N_f^k} \quad \text{where}\\
    \vec{\alpha}^k &= \left( \alpha_{1,1}^k,\dots, \alpha_{1,N_f^k}^k, \dots,
    \alpha_{L,1}^{k}, \dots, \alpha_{L,N_f^k}^k \right), \quad \text{with} \quad
    \alpha_{l,f}^k = \left(\alpha_{l,f}^{k(1)}, \alpha_{l,f}^{k(2)} \right) \in
    \R^2,
  \end{align}
  iterating over $Q$ subsystems first, then $L$ splines, then $N_f^k$ carrier wave
  frequencies. 

  In order to guarantee that the optimizer yields control pulses that are
  bounded with $|p_k(t)| \leq c^k_{max}$, $|q_k(t)| \leq c^k_{max}$ for all
  subsystems $k=1,\dots, Q$, we impose box constraints on the control
  parameters:
   \begin{align}
     | \alpha_{l,f}^{k(1)}| \leq \frac{c^k_{max}}{N_f^k} \quad \text{and} \quad |
     \alpha_{l,f}^{k(2)} | \leq \frac{c^k_{max}}{N_f^k}
   \end{align}
   where $N_f^k$ is the number of carrier wave frequencies.

  \subsubsection{Optimization algorithm}
    Quandary use Petsc's \textit{Tao} optimization package for solving
    the vectorized version of the optimal control problem. It uses the nonlinear Quasi-Newton
    optimization method that applies iterative preconditioned gradient steps using L-BFGS updates to approximate the Hessian. A projected line-search is applied to ensure that the control parameters stay within the prescribed box constraints and that the objective function yields sufficient decrease per iteration.



  \subsection{Parallelization (experimental)}
    Three levels of parallelization are planned, and partly implemented: 
      \begin{enumerate}
        \item Parallelization over initial conditions: working, configuration
          option \textit{np\_init}. Speedup over serial should be ideal, i.e.
          equal to \textit{np\_init}.
        \item Time-parallelization via XBraid: working implementation, but
          speedup depends on testcase and XBraid setting (experts only),
          configuration option is \textit{np\_braid}
        \item Spatial parallelism for parallel linear algebra (Petsc): working
          for the sparse-matrix solver, not for the tensor-contraction solver
      \end{enumerate}
      In the main code, the global communicator (MPI\_COMM\_WORLD) is split into
      three sub-communicator, one for each of the above. The total number of mpi
      processes ($np_{total}$) is split into three subgroups such that 
       \begin{align*}
         np_{braid} * np_{init} * np_{petsc} = np_{total}.
       \end{align*}
      The user specifies the size of the communicator for distributing the
      initial conditions ($np_{init}$) as well as time-parallelization
      ($np_{braid}$) in the config file. The number of cores for parallel linear
      algebra ($np_{petsc}$) is then computed from the above equation. The
      following requirements for parallel distribution must be considered when
      setting up parallel runs:
      \begin{itemize}
        \item $\frac{n_{init}}{np_{init}} \in \mathds{N}$, where $n_{init}$ is
          the number of initial conditions that are considered (being $N^2$ for
          the full basis, $N$ for considering diagonals only as initial
          condition, and $1$ if propagating only one initial condition e.g.
          reading from file, or pure state initialization). This requirement
          ensures that each processor group owns the same number of initial
          conditions.
        \item $\frac{np_{total}}{np_{init}*np_{braid}} \in \mathds{N}$, so that
          each processor group has the same number of cores for petsc.
        \item $\frac{N^2}{np_{petsc}} \in \mathds{N}$, hence the system
          dimensions must be integer multiple of the number of cores for
          distributing linear algebra in Petsc. This constraint is a little
          annoying, however the current implementation requires this due to the
          co-located storage of the real and imaginary parts of the vectorized
          state.
      \end{itemize}
  
\end{document}
