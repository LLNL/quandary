\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{authblk}
% \usepackage[legalpaper, margin=2in]{geometry}
\parindent0pt
\parskip 1.5ex plus 1ex minus .5ex

\definecolor{Blue}{rgb}{0,0,1}                                                     
\definecolor{Red}{rgb}{1,0,0}                                                      
\definecolor{Green}{rgb}{0,1,0}                                                    
\definecolor{Bronze}{rgb}{0.8,0.5,0.2}                                             
\definecolor{Violet}{rgb}{0.54,0.17,0.89}                                          
                                                                                   
\newcommand{\TODO}[1]{{\textcolor{Violet}{TODO: #1}}}                              
\newcommand{\YC}[1]{{\textcolor{Bronze}{#1}}}                                     
\newcommand{\SG}[1]{{\textcolor{Blue}{#1}}}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\Ell}{\mathcal{L}}
\newcommand{\R}{\mathds{R}}
\newcommand{\C}{\mathds{C}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\title{Quandary: Optimal Control for Quantum Systems}
\author{Stefanie G{\"u}nther\thanks{Center for Applied Scientific Computing, Lawrence Livermore
    National Laboratory, Livermore, CA, USA.} \and N. Anders Petersson$^*$} 
\date{December 29, 2020}

\begin{document}
\maketitle

\section*{Disclaimer}
This document was prepared as an account of work sponsored by an agency of the United States
government. Neither the United States government nor Lawrence Livermore National Security, LLC,
nor any of their employees makes any warranty, expressed or implied, or assumes any legal
liability or responsibility for the accuracy, completeness, or usefulness of any information,
apparatus, product, or process disclosed, or represents that its use would not infringe
privately owned rights. Reference herein to any specific commercial product, process, or service
by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply
its endorsement, recommendation, or favoring by the United States government or Lawrence
Livermore National Security, LLC. The views and opinions of authors expressed herein do not
necessarily state or reflect those of the United States government or Lawrence Livermore
National Security, LLC, and shall not be used for advertising or product endorsement purposes.


\section{Introduction}
Quandary numerically simulates and optimizes the time-evolution of open quantum systems. The
underlying dynamics are modeled by Lindblad's master equation, a linear ordinary differential
equation (ODE) describing quantum systems interacting with the environment. Quandary solves this ODE
numerically by applying a time-stepping integration scheme and applies a gradient-based optimization
scheme to determine optimal control pulses that drive the quantum system to a desired target state.
Two optimization objectives are considered: (a) Unitary gate optimization that finds controls to
realize a logical quantum gate, and (b) optimal reset that aims to drive the quantum system to the
ground states.
% Gradient-based optimization schemes utilizing Petsc's Tao optimization package are applied to
% generate control pulses that minimize the respective measure.
To mitigate excessive execution run times, Quandary can be build to link with the XBraid software
library which provides a parallelization strategy to distribute the time-evolution of the underlying
dynamics onto multiple processor applying a parallel-in-time multigrid reduction scheme.

This document outlines the mathematical background and underlying equations, and summarizes their
implementation in Quandary. A full documentation is under development. In the meantime, don't
hesitate to direct any questions to guenther5@llnl.gov. For installation instructions, please take a
look at the README.md document in the \verb+quandary+ directory.

\section{Model equation}
Quandary models open quantum systems with $Q$ subsystems with $n_k$ levels for the
$k$-th subsystem, $k=0,\dots,Q-1$. The time-evolution of the open quantum system is modeled by Lindblad's master equation:
\begin{align}\label{mastereq}
  \dot \rho(t) = &-i(H(t)\rho(t) - \rho(t)H(t)) \notag \\
  &+ \sum_{l=1}^2 \sum_{k=0}^{Q-1} \gamma_{lk} \left( \Ell_{lk} \rho(t)
  \Ell_{lk}^{\dagger} - \frac 1 2 \left( \Ell_{lk}^{\dagger}\Ell_{lk}
  \rho(t) + \rho(t)\Ell_{lk}^{\dagger} \Ell_{lk}\right) \right)
\end{align}
for the density matrix $\rho(t)\in \C^{N\times N}$ with dimension $N :=
\prod_{k=0}^{Q-1} n_k$. The Hamiltonian $H(t)$ consists of a constant \textit{drift}
part, and a time-varying \textit{control} part. For the computational
\textit{rotating frame} with fundamental frequencies $\omega_k$ and rotation
frequency $\omega_k^{\text{rot}}$, these are computed from
\begin{align}
  H(t) &= H_d + H_c(t) \\
  \text{with} \quad H_d &:= \sum_{k=0}^{Q-1} \left(\left(\omega_k -                                 
  \omega_k^{\text{rot}}\right)a_k^{\dagger}a_k- \frac{\xi_k}{2}
  a_k^{\dagger}a_k^{\dagger}a_k a_k - \sum_{l> k} \xi_{lk}
  a_{k}^{\dagger}a_{k}
  a_{l}^{\dagger} a_{l} \right) \\
   H_c(t) &:= \sum_{k=0}^{Q-1} \left( p^k(\vec{\alpha}^k,t) (a_k +
   a_k^{\dagger}) + i q^k(\vec{\alpha}^k,t)(a_k - a_k^{\dagger})
   \right)
\end{align}
Here, $a_k$ denotes the lowering operator:
\begin{align}
  \begin{array}{rl}
  a_0 &:= a^{(n_0)} \otimes I_{n_1} \otimes \dots \otimes
  I_{n_{Q-1}}\\
  a_1 &:= I_{n_0} \otimes a^{(n_1)} \otimes \dots \otimes
  I_{n_{Q-1}}\\
  \vdots \, & \\
  a_{Q-1} &:= I_{n_0} \otimes I_{n_1} \otimes \dots \otimes
  a^{(n_{Q-1})}\\
  \end{array}
  \quad \text{with}\quad
 a^{(n_k)} = \begin{pmatrix}
   0 & 1 &          &         &    \\
     & 0 & \sqrt{2} &         &     \\
     &   & \ddots   & \ddots  &    \\
     &   &          &         & \sqrt{n_k-1}  \\
     &   &          &         & 0   
 \end{pmatrix} \in \R^{n_k \times n_k}
\end{align}
and $I_{n_k}$ denotes the identity matrix in $\R^{n_k \times n_k}$.

\subsection{Collapse operators}
The collapse operators $\Ell_{lk}$ in the Lindblad terms of the master equation
\eqref{mastereq} can be of the following type:
\begin{itemize}
  \item ``T1'' -- Decay: $\Ell_{1k} = a_k$
  \item ``T2'' -- Dephasing: $\Ell_{2k} = a_k^{\dagger}a_k$
\end{itemize}
for each subsystem $k$. The constants $\gamma_{lk}$ are the inverse half-life
for the corresponding collapse process $l$: $\gamma_{lk} = {\frac{1}{T_l}}$.
Typical T1 decay time is between $10-100$ microseconds (us). T2
dephasing time is typically about half of T1 decay time. Decay then behaves
like $\exp(-t/{T_1})$. 

\subsection{Control pulses} \label{subsec:controlpulses}
The time-dependent control functions $p^k(\vec{\alpha}^k,t),
q^k(\vec{\alpha}^k,t)$ are real-valued. They are discretized using piecewise quadratic B-spline basis functions with
carrier waves: 
\begin{align}
  p^k(\vec{\alpha}^k,t) &= \sum_{l=0}^{L-1} B_l(t)
  \sum_{f=0}^{N_f^k-1} \left(\alpha^{k
  (0)}_{l,f} \cos(\Omega_f^k t) - \alpha^{k (1)}_{l,f} \sin(\Omega_f^k t)
  \right) \\
  q^k(\vec{\alpha}^k,t) &= \sum_{l=0}^{L-1} B_l(t)
  \sum_{f=0}^{N_f^k-1} \left( \alpha^{k
  (0)}_{l,f} \sin(\Omega_f^k t) + \alpha^{k (1)}_{l,f} \cos(\Omega_f^k t)
  \right)
\end{align}
for $L$ B-Spline functions $B_l(t)$, and $N_f^k$ carrier wave frequencies
$\Omega_f^k$ per oscillator. The amplitudes $\vec{\alpha}^k \in \R^{2LN_f^k}$ are the control
parameters (\textit{design} variables) that Quandary can optimize in order to realize a
desired system behavior. 

These control functions are in the \textit{rotating frame}. To convert them back
to the \textit{Lab frame}, Quandary uses
\begin{align}
  f^k(t) = 2 \sum_{l=0}^{L-1} B_l(t) \sum_{f=0}^{N_f^k-1} \beta_{l,f}^k \cos(w_k t +
  \Omega_f^k t + \theta_{l,f}) \quad \forall k=0,\dots Q-1
\end{align}
where $\beta_{l,f}^k$ and $\theta_{l,f}$ are computed from
\begin{align}
  \alpha_{l,f}^{k(0)} = \beta_{l,f}^k \cos(\theta_{l,f}) \quad \text{and} \quad
  \alpha_{l,f}^{k(1)} = \beta_{l,f}^k \sin(\theta_{l,f})
\end{align}
and $w_k$ are the fundamental resonance frequencies of subsystems $k$. Using
trigonometric equalities, the Lab-frame controls can be expressed as
\begin{align}
  f^k(t) &= 2 \sum_{l=1}^L B_l(t) \sum_{f=1}^{N_f^k} \alpha_{l,f}^{k(0)} \cos((w_k
  + \Omega_f^k) t) - \alpha_{l,f}^{k(1)}\sin((w_k + \Omega_f^k) t) \quad \forall
  k=0,\dots Q-1 \\
         &= 2 p^k(\vec{\alpha}^k, t) \cos(w_k t) - 2 q^k(\vec{\alpha}^k,
         t)\sin(w_k t) \\
\end{align}


\section{Optimization problem}
In the most general form, Quandary can solve the following optimization problem
\begin{align}
  \min_{\boldsymbol{\alpha}} \frac{1}{ninit} \sum_{i=1}^{ninit} \beta_i J(\rho_i(T))  + R(\boldsymbol{\alpha})
\end{align}
where $\rho_i(T)$ solves the master equation \eqref{mastereq} for initial conditions $\rho_i(0)$, as explained below. Specific choices of $J$ and initial conditions $\rho_i(0)$ are given below, as well as regularization and penalty terms denoted by $R$ above. 

\subsection{Ground-state optimization aka optimal
reset}\label{sec:groundstate-obj}
Optimal reset aims to find control pulses $p^k(\vec{\alpha}^k, t), q^k(\vec{\alpha}^k, t)$,
that drive any initial state towards the ground state $|0\dots 0\rangle = (1, 0,
0, \dots )^T$. The corresponding ground state density matrix is given by
\begin{align}
  \rho_{G} := |0\dots 0\rangle \langle 0 \dots 0 | = 
  \begin{bmatrix} 1      & 0      &  \dots   \\ 
                  0      & 0      &  \dots  \\ 
                  \vdots & \vdots &  \dots 
  \end{bmatrix}
\end{align}

To reach this goal, Quandary either minimizes the distance between the final state
density matrix and the ground-state density matrix, measured in the Frobenius
norm, or minimizes a weighted average of the expected energy level over each subsystem:
\begin{enumerate}
  \item \textit{Expected energy minimization}: Minimize a weighted sum over
    expected energy levels of the (full or partial) system
    \begin{align}
       \quad J(\rho(T)) = \sum_{k\in \mathcal{K}} \langle O^k \rangle 
    \end{align}
    using the observable $O^k = a_k^\dag a_k$ for the subsystem $k$, and defining a partial system
    $H_{k_0}\otimes \dots \otimes H_{k_s}$ from an index set of
    subsystem IDs $\mathcal{K}=\{k_0,\dots,k_s\}\subset
    \{0,\dots,Q-1\}$. 
    $O^k$ measures the probability of the energy levels of subsystem $k$. The
   expected energy level of subsystem $k$ is hence given by 
  \begin{align}
    \langle O^k \rangle &= \mbox{Tr}(O^k\rho).
    \label{eq:expected_energy1}
  \end{align}
    
    Quandary can use the following variations of the
    above measure
    \begin{enumerate}
      \item[(a)] $J(\rho(T)) =
        \left(\frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\langle O^k \rangle
        \right)^2$
      \item[(b)] $J(\rho(T)) =
        \frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\left(\langle O^k \rangle
        \right)^2 $
      \item[(c)] $J(\rho(T)) =
        \frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\langle O^k \rangle$
    \end{enumerate}
  \item \textit{Direct ground-state optimization}: Minimize the Frobenius norm
    between the final density matrix and the ground-state density matrix:
    \begin{align}\label{eq:ground-state-obj}
      J(\rho(T)) = \frac 12 \| \rho(T) - \rho_G \|^2_F 
    \end{align}
    where the Frobenius norm is defined as $\|A\|^2_F = \Tr(A^{\dagger}A)$.
\end{enumerate}

\subsection{Target gate optimization}

Here, the goal is to find control pulses that realize a certain logical gate
operation at final time $T$. Representing the gate by a unitary matrix $V\in
\C^{N\times N}$, the goal is to match the final state $\rho(T)$ to the unitary
transformation $V\rho(0)V^{\dagger}$ for any initial state $\rho(0)$. Two different measures are available:

\begin{align}
 \text{Frobenius norm:} \quad & J(\rho(T)) = \frac{1}{2}\| \rho(T) - V\rho(0)V^{\dagger} \|^2_F  \\
 \text{Trace overlap:} \quad  & J(\rho(T)) = 1 - \mbox{Tr}\left(V\rho(0)V^{\dagger}\rho(T) \right) 
\end{align} 


Target gates that are currently implemented are
\begin{align}
  V_{X} := \begin{bmatrix} 0 & 1 \\ 1 & 0  \end{bmatrix} \quad
  V_{Y} := \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} \quad
  V_{Z} := \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \quad 
  V_{Hadamard} := \frac{1}{\sqrt{2}} 
           \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \\
  V_{CNOT} := \begin{bmatrix} 1  & 0 & 0 & 0 \\ 
                               0  & 1 & 0 & 0 \\ 
                               0  & 0 & 0 & 1 \\ 
                               0  & 0 & 1 & 0 \\ 
                \end{bmatrix}
\end{align}

An average gate fidelity is computed from 
\begin{align}
  F_{avg} = \frac{1}{N^2} \sum_{i=1}^{N^2} \mbox{Tr}\left(V\rho_i(0)V^{\dagger}\rho_i(T) \right) 
\end{align}
where $\rho_i(0) = B^{k(i), j(i)}$ are the basis initial states and $\rho_i(T)$ their corresponding propagated final time states.

\subsection{Penalty and regularization term}
In order to stabilize the optimization problem, a Tikhonov regularization term can be added to the objective function:
\begin{align}
 \frac{\gamma_1}{2} \| \boldsymbol{\alpha} \|^2_2
\end{align}
with Tikhonov parameter $\gamma_1 > 0$, where $\boldsymbol{\alpha}$ denotes the vector of design parameters as in section \ref{subsec:controlpulses}.


In addition, when considering ground-state optimization in
Section~\ref{sec:groundstate-obj}, an integral penalty term can be added to the objective function that discourages non-zero energy states over time:
\begin{align}
  % \int_0^T w(t) J(\rho(t)) \, \mathrm{d}t, \quad \text{where} \quad w(t) =
  % \left(\frac{t}{T}\right)^p, p \geq 1
  \gamma_2 \int_0^T w(t) J\left(\rho(t)\right) \, \mathrm{d}t, \quad \text{where} \quad w(t) =
  \frac{1}{a} e^{ -\left(\frac{t-T}{a} \right)^2},
\end{align}
for a penalty parameter $\gamma_2 > 0$, and parameter $0 < a \leq 1$. 
Note, that as $a\to 0$, the weight $w(t)$ converges to the Dirac delta
distribution with peak at final time $T$. 



\subsection{Initial conditions}\label{subsec:initcond}

\subsubsection{Pure state initialization}
One can choose to simulate and optimize for only one specific initial condition $\rho(0)$. In that case, each evaluation of the objective function solves the master equation once for this specific initial condition ($\Rightarrow ninit = 1$). 

In the case of pure-state initialization, the initial density matrix is composed
of Kronecker products of pure states for each of the subsystems. E.g. for a bipartite system with $n_1
\otimes n_2$ levels, one can propagate any initial pure state 
\begin{align}
  \rho(0)  = |a\rangle \langle a| \otimes |b\rangle \langle b| \quad \text{for} \quad a \in \{0,\dots, n_1-1\}, b\in \{0,\dots, n_2-1\}
\end{align}
Alternatively, a specific initial condition can also be read from a file. 

The unique integer identifier for
this initial condition will be $i=-1$ (in order to distinguish from the below
basis elements or diagonals as below). 

\subsubsection{Basis for initial condition density matrices}
Typically, the optimization goal is to find control parameters that minimize the
respective measure for \textit{any} possible initial condition $\rho(0)$. We
therefore define a basis for all initial density matrices and aim to minimize
the average of the above measure over those matrix basis elements. The basis
matrices are defined as follows:

\begin{align}
B^{kj} := \frac 12 \left( E_{kk} + E_{jj}\right) +  \begin{cases} 
          0 & \text{if} \, k=j \\ 
        \frac 12 \left( E_{kj} + E_{jk}\right) & \text{if} \, k<j \\
        \frac i2 \left( E_{jk} - E_{kj}\right) & \text{if} \, k>j
      \end{cases} 
\end{align}
for all $k,j\in\{0,\dots, N-1\}$, where $E_{kj}$ denotes the matrix of all zeros
except for the $(k,j)$-th element which is one (hence $E_{kj} = e_ke_j^T$).
These $N^2$ matrices are Hermitian, and they are linearly independent over the
real vector space of Hermitian matrices. They further satisfy $\Tr(B^{kj}) = 1$
and they are positive semi-definite, hence each basis matrix is a density
matrix. 

In Quandary, these $N^2$ basis elements can be used to perform the optimization ($\Rightarrow ninit=N^2$). To uniquely identify the different initial conditions in the code, we assign a
unique index $i \in \{0,\dots, N^2-1\}$ to each basis elements with 
\begin{align*}
  B^i := B^{k(i), j(i)}, \quad \text{with} \quad k(i) := i \,\mbox{mod}\, N,
  \quad \text{and} \quad j(i) := \left\lfloor \frac{i}{N} \right\rfloor
\end{align*}

% We can rewrite any initial condition $\rho(0)$ as a linear combination 
% \begin{align}
%   \rho(0) = \sum_{k=0}^{N-1} \sum_{j=0}^{N-1} a_{kj} B^{k, j}.
% \end{align}
% for coefficients $a_{kj} \in \R$.
% Since the master equation \eqref{mastereq} is linear, we can formally define a
% linear solution operator $S^{\vec{\alpha}}(t,\rho(0))$, mapping an initial state
% to the state at time $t$, and we can rewrite the final density matrix at time
% $T$ in terms of the basis elements:
% \begin{align}
%   \rho(T) = S^{\vec{\alpha}}(T,\rho(0)) = \sum_{k,j=0}^{N-1} a_{kj}
%   S^{\vec{\alpha}}(T,B^{k, j}).
% \end{align}
% % AP: According to D.~Lidar's lectue notes, eqn. (112), the most general
% expression for the evolution of the density matrix follows from the Kraus
% operator sum representation,
% \[
%   \rho(T) = \sum_{\mu,\nu} K_{\mu\nu}(t) \rho(0) K_{\mu\nu}^\dagger(t),\quad
%   \sum_{\mu,\nu}
%   K_{\mu\nu}^\dagger(t)K_{\mu\nu}(t)=I.
% \]

% We therefore define the optimization problem that minimizes the respective
% measure averaged over all the basis elements:
% \begin{align}\label{optimproblem_basis}
%   \min \frac{1}{N^2} & \sum_{k,j=0}^{N-1} \, J(\rho^{k,j}(T))  \\
%   \text{s.t.} \quad  \text{each} \quad \rho^{k,j}(t) \quad & \text{solves
%   \eqref{mastereq} with initial condition} \quad \rho^{k,j}(0) = B^{k,j}
% \end{align}

% If the objective measure $J(\rho)$ is linear in $\rho$ (as for example in the
% groundstate optimization minimizing the expected energy levels), we can make use
% of the linearity of $S^{\vec{\alpha}}$:
% \begin{align}
%    \frac{1}{N^2}  \sum_{k,j=0}^{N-1} \, J\left(\rho^{k,j}(T)\right) =
%    J\left(S^{\vec{\alpha}}\left(T,\frac{1}{N^2}  \sum_{k,j=0}^{N-1}
%    B^{kj}\right)\right)
% \end{align}
% Hence, we define a new optimization problem using the specific initial condition
% that averages over the basis matrices:
% \begin{align}
%   \min \, J(S^{\vec{\alpha}}(T,\rho(0))) \quad \text{s.t.} \quad \rho(0) =
%   \frac{1}{N^2}  \sum_{k,j=0}^{N-1} B^{kj}
% \end{align}
% In this case, each objective function evaluation requires to propagate only one
% initial condition $\rho(0)$. 

\subsubsection{Diagonals as initial condition}
One can choose to propagate only those basis elements that correspond to pure states. In that case, only the basis matrices that correspond
to diagonal elements ($B^{mm}, m=1,\dots,N)$ are propagated through the time
domain, and the objective function evaluates the average measure over those $N$
initial conditions at the final time $T$ ($\Rightarrow ninit = N$).

% \subsection{Fidelity} 
% For closed systems, the $\rho(t)$ and the solution operator $U(t)$ are unitary. In that case, one can show that 
% \begin{align}
%   \frac{1}{2N^2}\|\rho(T) - V\rho(0)V\|_F^2 = 1 - \frac{1}{N^2}|\Tr(U(T)^{\dagger}V)|^2,
% \end{align}
% which is called the \textit{infidelity}, and the trace-term is the \textit{fidelity}. However, since we include the Lindblad terms, $\rho(T)$ is not unitary, and the above equation is not valid anymore. For defining a fidelity we consider $F(z) := 1 - J(z)$. Note, however, that if the optimized objective is in the order $O(\delta)$, then the error $\epsilon_i$ of a final state component $i$ from the gate is rather of the order $O(\sqrt{\delta})$, since $J$ represents the average squared error $J = \frac{1}{N^2}\sum_i \epsilon_i^2$ and so $\epsilon_i^2 \sim O(\delta)$ on average.

\subsection{Three-state initialization}
According to Koch et al. it is sufficient to utilize only three initial condition to describe a unitary gate, independent of the dimension $N$. Therefore, if one optimizes for realizing a unitary gate, the following three initial states can be chosen to be considered in Quandary:
\begin{align}
    \rho(0)_0 &= \sum_{i=0}^{N-1} \frac{2(N-i+1)}{N(N+1)} |i\rangle\langle i|\\
    \rho(0)_1 &= \sum_{ij=0}^{N-1} \frac{1}{N} |i\rangle\langle j|\\
    \rho(0)_2 &= \frac{1}{N} I_N
\end{align}


\subsection{Reading an initial condition from file}
A specific initial condition can be read from file ($\Rightarrow ninit=1$ with unique identifier $-1$). Format: one column being the vectorized density matrix, first all real parts, then all imaginary parts. 

\section{Implementation}

  \subsection{Vectorization}
  The Lindblad master equation \eqref{mastereq} is in matrix form, describing
  the evolution of the density matrix $\rho = (\rho_1, \dots, \rho_N) \in
  \C^{N\times N}$. In order to solve this numerically, we vectorize the equation
  to receive an ODE for $q(t) := \text{vec}(\rho(t)) \in \C^{N^2}$. Using the
  relations
  \begin{align}
   \text{vec}(AB) &= (I_N\otimes A)\text{vec}(B) = (B^T\otimes I_N)\text{vec}(A)
    \\
   \text{vec}(ABC) &= (C^T\otimes A)\text{vec}(B)
  \end{align}
  for square matrices $A,B,C\in\C^{N\times N}$, we can derive the vectorized
  form of the Lindblad master equation:
  \begin{align}\label{mastereq_vectorized}
    &\dot q(t) = M(t) q(t) \quad  \text{where} \\
    &M(t) := -i(I_N\otimes H - H^T \otimes I_N) + \sum_{k=0}^{Q-1}\sum_{l=1}^2 \gamma_{lk}
    \left( \Ell_{lk}\otimes \Ell_{lk} - \frac 1 2 \left( I_N\otimes
    \Ell^T_{lk}\Ell_{lk} + \Ell^T_{lk}\Ell_{lk} \otimes I_N \right) \right)
  \end{align}

  \subsection{Real-valued system}
   Quandary solves the vectorized master equation \eqref{mastereq_vectorized} in
   real-valued variables with $q(t) = u(t) + iv(t)$, evolving the real-valued
   states $u(t), v(t)\in \R^{N^2}$ with
   \begin{align}
     \dot q(t) = M(t) q(t) \quad \Leftrightarrow \quad \begin{bmatrix} \dot u(t) \\ \dot v(t) \end{bmatrix} = 
   \begin{bmatrix} A(t) & -B(t) \\ B(t) & A(t) \end{bmatrix} 
   \begin{pmatrix} u(t) \\ v(t) \end{pmatrix} 
   \label{realvaluedODE}
   \end{align}
   where $M(t) = A(t) + i B(t)$, $A(t), B(t)\in \R^{N^2\times N^2}$. To assemble
   $A = Re(M)$ and $B = Im(M)$ consider
   \begin{align}
     -i(I_N \otimes H - H^T \otimes I_N) &= -I_N \otimes \left(iH_d +
     iH_c(t)\right) + \left(iH_d + iH_c(t)\right)^T \otimes I_N \\
     \text{and} \quad iH_d + iH_c(t) &= i H_d + i\left( \sum_k
     p^k(\vec{\alpha}^k,t)(a_k + a_k^{\dagger}) + iq^k(\vec{\alpha}^k,t)(a_k -
     a_k^{\dagger})\right) \\
                    &= - \sum_k q^k(\alpha^k,t)(a_k - a_k^{\dagger}) + i\left(
                    H_d + \sum_k p^k(\alpha^k,t)(a_k+a_k^{\dagger}) \right) 
   \end{align}
   Hence $A$ and $B$ consist of a constant part $A_d, B_d$ and a time-varying
   part $A_c(t), B_c(t)$ with the following:
   \begin{align}
     A_d &=  \sum_{l,k}\gamma_{lk} \left( \Ell_{lk}\otimes\Ell_{lk} -
     \frac 1 2 \left(I_N \otimes \Ell_{lk}^T\Ell_{lk} +
     \Ell_{lk}^T\Ell_{lk}\otimes I_N\right) \right)\\
     B_d &= -I_N \otimes H_d + H_d^T \otimes I_N \\
     A_c(t) &= \sum_k q^k(\vec{\alpha}^k,t) \underbrace{\left( I_N \otimes
     \left(a_k - a_k^{\dagger}\right) - \left(a_k -
     a_k^{\dagger}\right)^T\otimes I_N \right)}_{=:A_c^k} \\
     B_c(t) &= \sum_k p^k(\vec{\alpha}^k,t) \underbrace{\left( - I_N \otimes
     \left(a_k + a_k^{\dagger}\right) + \left(a_k +
     a_k^{\dagger}\right)^T\otimes I_N \right)}_{=:B_c^k} 
   \end{align}

   \subsection{Sparse-matrix vs. matrix-free solver}

   In Quandary, two versions to evaluate the right hand side of Lindblad's
   equation, $M(t)q(t)$, of the vectorized real-valued system are available:
   \begin{itemize}
     \item \textit{Sparse-matrix solver:}
      The sparse-matrix solver initializes and stores the constant matrices
       $A_d,B_d, A_c^k, B_c^k$ using Petsc's sparse-matrix format. They are used
       as building blocks to evaluate the sparse system matrix $M(t)$ with 
     \begin{align}
       A(t) &= Re(M(t)) = A_d + \sum_kq^k(\alpha^k, t)A_c^k \\
       B(t) &= Im(M(t)) = B_d + \sum_k p^k(\alpha^k, t)B_c^k
     \end{align}
   at each time $t$. $M(t)$ is then multiplied to the vectorized density matrix
       $q(t)$ using Petsc's sparse MatVec implementation. 

   \item \textit{Matrix-free solver using tensor contractions:}
     Then tensor-contraction code is matrix-free, i.e. the matrices $A_d,B_d,
       A_c^k, B_c^k$ and hence $M(t)$ are not stored explicitly, but instead
       their action on a vector $q(t)$ is evaluated at each time step $t$. The
       action of those matrices to the system vector is implemented using
       tensor-contractions applied to the corresponding dimension of the density
       matrix. 

     \textbf{Notes}:
     \begin{itemize}
       \item For our current test cases, the matrix-free solver is much faster
         than the sparse-matrix solver (about 10x). However the matrix-free solver
         is currently only implemented for systems consisting of \textbf{two}
         subsystems with $n_1 \otimes n_2$ levels. Future development will
         extend the approach to more than two subsystem. 
       \item The matrix-free solver currently does not parallelize across the
         system dimension (i.e. no parallel Petsc!).
     \end{itemize}


   \end{itemize}

  \subsection{Colocated storage of real and imaginary part}
  The real and imaginary parts of $q(t)$ are stored in a colocated manner: For
  $q = u+iv$ with $u,v\in\R^{N^2}$, a vector of size $2N^2$ is stored that
  staggers real and imaginary parts behind each other for each component:
  \begin{align*}
    q = u+iv = \begin{bmatrix}
     u^1\\u^2\\ \vdots \\ u^{N^2-1} 
    \end{bmatrix}
    + i \begin{bmatrix}
     v^1\\v^2\\ \vdots \\ v^{N^2-1} 
    \end{bmatrix}
    \quad \Rightarrow \quad
    q_{store} = \begin{bmatrix}
      u_1 \\ v_1\\ u_2 \\ v_2 \\ \vdots \\ u_{N^2-1} \\ v_{N^2-1}
    \end{bmatrix}
  \end{align*}
  In order to access the real and imaginary parts of the $i^{th}$ component of
  $q$, one has to access the elements of $q_{store}$ at index $2i$ and $2i+1$,
  respectively. 

  \subsection{Time-stepping}
    To solve the vectorized master equation \eqref{mastereq_vectorized}, $\dot
    q(t) = M(t) q(t)$ for $t\in [0,T]$, Quandary applies a time-stepping integration
    scheme on a uniform time discretization grid $0=t_0 < \dots t_{N} = T$, with
    $t_n = n \delta t$ and $\delta t = \frac{T}{N}$, and approximates the
    solution at each discrete time step $q^{n} \approx q(t_n)$. The implicit midpoint rule is the default setting, and is the preferred time-stepping integration scheme. 
   
    \subsubsection{Implicit Midpoint Rule (IMR)} 
    The implicit mid-point rule is a symplectic, second-order integration scheme
    of Runge-Kutta type, with a Runge-Kutta tableau given by
    \begin{tabular}{ c | c }
      $1/2$ & $ 1/2$ \\
      \hline
                &  $1$
    \end{tabular}.
    Given a state $q^n$ at time $t_n$, the update formula to compute $q^{n+1}$
    is hence 
    \begin{align}
      q^{n+1} = q^n + \delta t k_1 \quad \text{where} \, k_1 \, \text{solves}
      \quad \left( I-\frac{\delta t}{2} M^{n+1/2} \right) k_1 = M^{n+1/2}  q^n
    \end{align}
    where $M^{n+1/2} := M(t_n + \frac{\delta t}{2})$. In each time-step, we
    first solve a linear equation to get the stage variable $k_1$, then use it
    to update $q^{n+1}$. 

    The consistent discrete adjoint (backwards) time-integration step for
    adjoint variables $\bar q^{n+1}, \bar q^n$, and the gradient update for this
    step are given by
    \begin{align}
      \bar q^{n} = \bar q^{n+1} + \delta t \left(M^{n+1/2}\right)^T \bar k_1
      \quad \text{where} \, \bar k_1 \, \text{solves} \quad \left(
      I-\frac{\delta t}{2} M^{n+1/2}\right)^T  \bar k_1 = \bar q^{n+1} 
    \end{align}
    The contribution to the reduced gradient $\nabla J$ for each time step is
    then given by
    \begin{align}
      \nabla J += \delta t \left( \frac{\partial M^{n+1/2}}{\partial z}
      \left(q^n + \frac{\delta t}{2} k_1\right) \right)^T\bar k_1
    \end{align}


    \subsubsection{Other integrators}
    The code offers the possibility to apply any time-stepping integrators
    provided by the Petsc software package (class TS). Implementation needs to
    be verified. Adjoint time-stepping with Petsc is currently not
    implemented. 

    \subsubsection{Choice of the time-step size}
    In order to choose a time-step size $\delta t$, an eigenvalue analysis of
    the constant drift Hamiltonian $H_d =  -2\pi \left(\sum_{k=0}^{Q-1} \frac{x_k}{2}
    a_k^{\dagger}a_k^{\dagger}a_ka_k + \sum_{l>k} x_{lk}
    a_{k}^{\dagger}a_{k}
    a_{l}^{\dagger}a_{l}\right)$ is considered:
       \begin{align*}  
         \dot u = -i H_d u \qquad \text{with} \quad H_d^{\dagger}  = H_d
       \end{align*} 
       Since $H_d$ is Hermitian, there exists a transformation $Y$ s.t. 
       \begin{align*}
         Y^{\dagger}H_d Y = \Lambda \qquad  \text{where} \quad Y^{\dagger} = Y
       \end{align*}
       where $\Lambda$ is a diagonal matrix containing the eigenvalues of $H_d$.
       Transform $\tilde u = Y^{\dagger} u$, then the ODE transforms to 
       \begin{align*}
         \dot \tilde u = -i \Lambda \tilde u \quad \Rightarrow \dot \tilde u_i =
         -i\lambda_i \tilde u_i \quad \Rightarrow \tilde u_i = a
         \exp(-i\lambda_i t)
       \end{align*}
       Therefore, the period for each mode is $\tau_i =
       \frac{2\pi}{|\lambda_i|}$, hence the shortest period is $\tau_{min} =
       \frac{2\pi}{\max_i\{|\lambda_i|\}}$. If we want $p$ discrete time points
       per period, then $p\delta t = \tau_{min}$, hence 
       \begin{align*}
         \delta t = \frac{\tau_{min}}{p} = \frac{2\pi}{p\max_i\{|\lambda_i|\}}
       \end{align*}
       Usually, for a first order scheme we would use something like $p=20$,
       second order may use $p=10$. 

       If we want to include the time-varying Hamiltonian part $H = H_d +
       H_c(t)$ in the analysis, then we could use the constraints on the control
       parameter amplitudes to remove the time-dependency using their large
       value instead, and then do the same analysis as above. However this
       doesn't ensure that we resolve the time-scale of the fastest control
       function. 
      
\subsection{Optimization}

\subsubsection{Optimization variables and bounds}

Design parameters (optimization variables) are stored in the Quandary code in the following order: List oscillators first $(\vec{\alpha}^1, \dots, \vec{\alpha}^Q)$, then for each $\vec{\alpha}^k \in
\R^{2LN_f^k}$ iterate over $L$ splines: $\vec{\alpha}^k =
(\alpha^k_1,\dots, \alpha^k_{L})$ with $\alpha^k_l \in \R^{2N_f^k}$, then each
$\alpha^k_l$ iterates over carrier waves and for each carrier wave lists
the two components: $\alpha^k_l = \alpha^{k(1)}_{l,1}, \alpha^{k(2)}_{l,1},
\dots \alpha^{k(1)}_{l,N_f^k}, \alpha^{k(2)}_{l,N_f^k}$. Hence there are a total of $2QL\sum_k N_f^k$ optimization parameters, which are stored in the following order:
  \begin{align}
    \boldsymbol{\alpha} &:= \left( \vec{\alpha}^1, \dots, \vec{\alpha}^Q \right), \in
    \mathds{R}^{2QL\sum_k N_f^k} \quad \text{where}\\
    \vec{\alpha}^k &= \left( \alpha_{1,1}^k,\dots, \alpha_{1,N_f^k}^k, \dots,
    \alpha_{L,1}^{k}, \dots, \alpha_{L,N_f^k}^k \right), \quad \text{with} \quad
    \alpha_{l,f}^k = \left(\alpha_{l,f}^{k(1)}, \alpha_{l,f}^{k(2)} \right) \in
    \R^2,
  \end{align}
  iterating over $Q$ subsystems first, then $L$ splines, then $N_f^k$ carrier wave
  frequencies. 

  In order to guarantee that the optimizer yields control pulses that are
  bounded with $|p_k(t)| \leq c^k_{max}$, $|q_k(t)| \leq c^k_{max}$ for all
  subsystems $k=1,\dots, Q$, we impose box constraints on the control
  parameters:
   \begin{align}
     | \alpha_{l,f}^{k(1)}| \leq \frac{c^k_{max}}{N_f^k} \quad \text{and} \quad |
     \alpha_{l,f}^{k(2)} | \leq \frac{c^k_{max}}{N_f^k}
   \end{align}
   where $N_f^k$ is the number of carrier wave frequencies.

  \subsubsection{Optimization algorithm}
    Quandary use Petsc's \textit{Tao} optimization package for solving
    the vectorized version of the optimal control problem. It uses the nonlinear Quasi-Newton
    optimization method that applies iterative preconditioned gradient steps using L-BFGS updates to approximate the Hessian. A projected line-search is applied to ensure that the control parameters stay within the prescribed box constraints and that the objective function yields sufficient decrease per iteration.



    \subsection{Parallelization (experimental)}
    Three levels of parallelization are planned, and partly implemented: 
    \begin{enumerate}
    \item Parallelization over initial conditions: working, configuration
      option \textit{np\_init}. Speedup over serial should be ideal, i.e.
      equal to \textit{np\_init}.
    \item Time-parallelization via XBraid: working implementation, but
      speedup depends on the test case and XBraid setting (experts only),
      configuration option is \textit{np\_braid}
    \item Spatial parallelism for parallel linear algebra (Petsc): working
      for the sparse-matrix solver, not for the tensor-contraction solver
    \end{enumerate}
    In the main code, the global communicator (MPI\_COMM\_WORLD) is split into
    three sub-communicator, one for each of the above. The total number of MPI
    processes ($np_{total}$) is split into three subgroups such that 
    \begin{align*}
      np_{braid} * np_{init} * np_{petsc} = np_{total}.
    \end{align*}
    The user specifies the size of the communicator for distributing the
    initial conditions ($np_{init}$) as well as time-parallelization
    ($np_{braid}$) in the config file. The number of cores for parallel linear
    algebra ($np_{petsc}$) is then computed from the above equation. The
    following requirements for parallel distribution must be considered when
    setting up parallel runs:
    \begin{itemize}
    \item $\frac{n_{init}}{np_{init}} \in \mathds{N}$, where $n_{init}$ is
      the number of initial conditions that are considered (being $N^2$ for
      the full basis, $N$ for considering diagonals only as initial
      condition, and $1$ if propagating only one initial condition e.g.
      reading from file, or pure state initialization). This requirement
      ensures that each processor group owns the same number of initial
      conditions.
    \item $\frac{np_{total}}{np_{init}*np_{braid}} \in \mathds{N}$, so that
      each processor group has the same number of cores for Petsc.
    \item $\frac{N^2}{np_{petsc}} \in \mathds{N}$, hence the system
      dimensions must be integer multiple of the number of cores for
      distributing linear algebra in Petsc. This constraint is a little
      annoying, however the current implementation requires this due to the
      colocated storage of the real and imaginary parts of the vectorized
      state.
    \end{itemize}

    \section*{Acknowledgments}
    This work was performed under the auspices of the U.S. Department of Energy by Lawrence
    Livermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-SM-818073. 

\end{document}
